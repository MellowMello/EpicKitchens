{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EpicKitchens.ipynb",
      "provenance": [],
      "mount_file_id": "1DeIYw_t8XytFs0PW0P1Mew7Q6pcR0FJ7",
      "authorship_tag": "ABX9TyOUsZ2gmYXHisVuVgQPsu85",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MellowMello/EpicKitchens/blob/main/EpicKitchens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59AiacFb9B6w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "15c25e4d-2c54-4144-8746-b06ace905d13"
      },
      "source": [
        "pip install pretrainedmodels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.6/dist-packages (0.7.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (0.7.0+cu101)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (2.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (1.6.0+cu101)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from munch->pretrainedmodels) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la3LaTjikqbx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e09958e4-4a3e-4c95-a981-a35b76018c76"
      },
      "source": [
        "import torch.hub\n",
        "repo = 'epic-kitchens/action-models'\n",
        "\n",
        "class_counts = (125, 352)\n",
        "segment_count = 8\n",
        "base_model = 'resnet50'\n",
        "tsn = torch.hub.load(repo, 'TSN', class_counts, segment_count, 'RGB',\n",
        "                     base_model=base_model, \n",
        "                     pretrained='epic-kitchens', force_reload=True)\n",
        "trn = torch.hub.load(repo, 'TRN', class_counts, segment_count, 'RGB',\n",
        "                     base_model=base_model, \n",
        "                     pretrained='epic-kitchens')\n",
        "mtrn = torch.hub.load(repo, 'MTRN', class_counts, segment_count, 'RGB',\n",
        "                     base_model=base_model, \n",
        "                      pretrained='epic-kitchens')\n",
        "#tsm = torch.hub.load(repo, 'TSM', class_counts, segment_count, 'RGB',\n",
        " #                    base_model=base_model, \n",
        "  #                   pretrained='epic-kitchens')\n",
        "\n",
        "# Show all entrypoints and their help strings\n",
        "for entrypoint in torch.hub.list(repo):\n",
        "    print(entrypoint)\n",
        "    print(torch.hub.help(repo, entrypoint))\n",
        "\n",
        "batch_size = 1\n",
        "segment_count = 8\n",
        "snippet_length = 1  # Number of frames composing the snippet, 1 for RGB, 5 for optical flow\n",
        "snippet_channels = 3  # Number of channels in a frame, 3 for RGB, 2 for optical flow\n",
        "height, width = 224, 224\n",
        "\n",
        "inputs = torch.randn(\n",
        "    [batch_size, segment_count, snippet_length, snippet_channels, height, width]\n",
        ")\n",
        "# The segment and snippet length and channel dimensions are collapsed into the channel\n",
        "# dimension\n",
        "# Input shape: N x TC x H x W\n",
        "inputs = inputs.reshape((batch_size, -1, height, width))\n",
        "for model in [tsn, trn, mtrn]: #, tsm]:\n",
        "    # You can get features out of the models\n",
        "    features = model.features(inputs)\n",
        "    # and then classify those features\n",
        "    verb_logits, noun_logits = model.logits(features)\n",
        "    \n",
        "    # or just call the object to classify inputs in a single forward pass\n",
        "    verb_logits, noun_logits = model(inputs)\n",
        "    print(verb_logits.shape, noun_logits.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/epic-kitchens/action-models/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "Using cache found in /root/.cache/torch/hub/epic-kitchens_action-models_master\n",
            "Using cache found in /root/.cache/torch/hub/epic-kitchens_action-models_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Multi-Scale Temporal Relation Network Module in use ['8-frame relation', '7-frame relation', '6-frame relation', '5-frame relation', '4-frame relation', '3-frame relation', '2-frame relation']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/epic-kitchens_action-models_master\n",
            "Using cache found in /root/.cache/torch/hub/epic-kitchens_action-models_master\n",
            "Using cache found in /root/.cache/torch/hub/epic-kitchens_action-models_master\n",
            "Using cache found in /root/.cache/torch/hub/epic-kitchens_action-models_master\n",
            "Using cache found in /root/.cache/torch/hub/epic-kitchens_action-models_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MTRN\n",
            "\n",
            "    Multi-scale Temporal Relational Network\n",
            "\n",
            "    See https://arxiv.org/abs/1711.08496 for more details.\n",
            "    Args:\n",
            "        num_class:\n",
            "            Number of classes, can be either a single integer,\n",
            "            or a 2-tuple for training verb+noun multi-task models\n",
            "        num_segments:\n",
            "            Number of frames/optical flow stacks input into the model\n",
            "        modality:\n",
            "            Either ``RGB`` or ``Flow``.\n",
            "        base_model:\n",
            "            Backbone model architecture one of ``resnet18``, ``resnet30``,\n",
            "            ``resnet50``, ``BNInception``, ``InceptionV3``, ``VGG16``.\n",
            "            ``BNInception`` and ``resnet50`` are the most thoroughly tested.\n",
            "        new_length:\n",
            "            The number of channel inputs per snippet\n",
            "        consensus_type:\n",
            "            The consensus function used to combined information across segments.\n",
            "            One of ``avg``, ``max``, ``TRN``, ``TRNMultiscale``.\n",
            "        before_softmax:\n",
            "            Whether to output class score before or after softmax.\n",
            "        dropout:\n",
            "            The dropout probability. The dropout layer replaces the backbone's\n",
            "            classification layer.\n",
            "        img_feature_dim:\n",
            "            Only for TRN/MTRN models. The dimensionality of the features used for\n",
            "            relational reasoning.\n",
            "        partial_bn:\n",
            "            Whether to freeze all BN layers beyond the first 2 layers.\n",
            "        pretrained:\n",
            "            Either ``'imagenet'`` for ImageNet initialised models,\n",
            "            or ``'epic-kitchens'`` for weights pretrained on EPIC-Kitchens.\n",
            "    \n",
            "TRN\n",
            "\n",
            "    Single-scale Temporal Relational Network\n",
            "\n",
            "    See https://arxiv.org/abs/1711.08496 for more details.\n",
            "    Args:\n",
            "        num_class:\n",
            "            Number of classes, can be either a single integer,\n",
            "            or a 2-tuple for training verb+noun multi-task models\n",
            "        num_segments:\n",
            "            Number of frames/optical flow stacks input into the model\n",
            "        modality:\n",
            "            Either ``RGB`` or ``Flow``.\n",
            "        base_model:\n",
            "            Backbone model architecture one of ``resnet18``, ``resnet30``,\n",
            "            ``resnet50``, ``BNInception``, ``InceptionV3``, ``VGG16``.\n",
            "            ``BNInception`` and ``resnet50`` are the most thoroughly tested.\n",
            "        new_length:\n",
            "            The number of channel inputs per snippet\n",
            "        consensus_type:\n",
            "            The consensus function used to combined information across segments.\n",
            "            One of ``avg``, ``max``, ``TRN``, ``TRNMultiscale``.\n",
            "        before_softmax:\n",
            "            Whether to output class score before or after softmax.\n",
            "        dropout:\n",
            "            The dropout probability. The dropout layer replaces the backbone's\n",
            "            classification layer.\n",
            "        img_feature_dim:\n",
            "            Only for TRN/MTRN models. The dimensionality of the features used for\n",
            "            relational reasoning.\n",
            "        partial_bn:\n",
            "            Whether to freeze all BN layers beyond the first 2 layers.\n",
            "        pretrained:\n",
            "            Either ``'imagenet'`` for ImageNet initialised models,\n",
            "            or ``'epic-kitchens'`` for weights pretrained on EPIC-Kitchens.\n",
            "    \n",
            "TSM\n",
            "\n",
            "    Temporal Shift Module\n",
            "\n",
            "    See https://arxiv.org/abs/1811.08383 for details.\n",
            "\n",
            "    Args:\n",
            "        num_class:\n",
            "            Number of classes, can be either a single integer,\n",
            "            or a 2-tuple for training verb+noun multi-task models\n",
            "        num_segments:\n",
            "            Number of frames/optical flow stacks input into the model\n",
            "        modality:\n",
            "            Either ``RGB`` or ``Flow``.\n",
            "        base_model:\n",
            "            Backbone model architecture one of ``resnet18``, ``resnet30``,\n",
            "            ``resnet50``, ``BNInception``.\n",
            "        new_length:\n",
            "            The number of channel inputs per snippet\n",
            "        consensus_type:\n",
            "            The consensus function used to combined information across segments.\n",
            "            One of ``avg``, ``max``, ``TRN``, ``TRNMultiscale``.\n",
            "        before_softmax:\n",
            "            Whether to output class score before or after softmax.\n",
            "        dropout:\n",
            "            The dropout probability. The dropout layer replaces the backbone's\n",
            "            classification layer.\n",
            "        img_feature_dim:\n",
            "            Only for TRN/MTRN models. The dimensionality of the features used for\n",
            "            relational reasoning.\n",
            "        partial_bn:\n",
            "            Whether to freeze all BN layers beyond the first 2 layers.\n",
            "        shift_div:\n",
            "            The reciprocal of the proportion of channels that will be shifted\n",
            "            along the time dimension.\n",
            "        shift_place:\n",
            "            Either ``'blockres'`` or ``'block'``. The former will place the shift\n",
            "            module in the residual branch (only compatible with ResNet derived\n",
            "            backbones), and the latter will place the shift module on the main\n",
            "            network path.\n",
            "        fc_lr5:\n",
            "            Whether to add a x5 multiplier to the the fully connected layer\n",
            "        temporal_pool:\n",
            "            Whether to gradually temporally pool throughout the network\n",
            "        non_local:\n",
            "            Whether to inject non-local blocks\n",
            "        pretrained:\n",
            "            Either ``'imagenet'`` for ImageNet initialised models,\n",
            "            or ``'epic-kitchens'`` for weights pretrained on EPIC-Kitchens.\n",
            "    \n",
            "TSN\n",
            "\n",
            "    Temporal Segment Network\n",
            "\n",
            "    See https://arxiv.org/abs/1608.00859 for more details.\n",
            "\n",
            "    Args:\n",
            "        num_class:\n",
            "            number of classes, can be either a single integer,\n",
            "            or a 2-tuple for training verb+noun multi-task models\n",
            "        num_segments:\n",
            "            number of frames/optical flow stacks input into the model\n",
            "        modality:\n",
            "            either ``rgb`` or ``flow``.\n",
            "        base_model:\n",
            "            backbone model architecture one of ``resnet18``, ``resnet30``,\n",
            "            ``resnet50``, ``bninception``, ``inceptionv3``, ``vgg16``.\n",
            "            ``bninception`` and ``resnet50`` are the most thoroughly tested.\n",
            "        new_length:\n",
            "            the number of channel inputs per snippet\n",
            "        consensus_type:\n",
            "            the consensus function used to combined information across segments.\n",
            "            one of ``avg``, ``max``, ``trn``, ``trnmultiscale``.\n",
            "        before_softmax:\n",
            "            whether to output class score before or after softmax.\n",
            "        dropout:\n",
            "            the dropout probability. the dropout layer replaces the backbone's\n",
            "            classification layer.\n",
            "        img_feature_dim:\n",
            "            only for trn/mtrn models. the dimensionality of the features used for\n",
            "            relational reasoning.\n",
            "        partial_bn:\n",
            "            whether to freeze all bn layers beyond the first 2 layers.\n",
            "        pretrained:\n",
            "            either ``'imagenet'`` for imagenet initialised models,\n",
            "            or ``'epic-kitchens'`` for weights pretrained on epic-kitchens.\n",
            "    \n",
            "torch.Size([1, 125]) torch.Size([1, 352])\n",
            "torch.Size([1, 125]) torch.Size([1, 352])\n",
            "torch.Size([1, 125]) torch.Size([1, 352])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03D22axm9lvO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "69e120be-9a84-464e-c359-a7450b191d82"
      },
      "source": [
        "from tsn import TSN, TRN, MTRN\n",
        "import torch\n",
        "\n",
        "#model_path = '/content/drive/My Drive/2tw6gdvmfj3f12papdy24flvmo/TSN_arch=resnet50_modality=RGB_segments=8-3ecf904f.pth.tar'\n",
        "model_path = '/content/drive/My Drive/2tw6gdvmfj3f12papdy24flvmo/MTRN_arch=resnet50_modality=RGB_segments=8-46337796.pth.tar'\n",
        "\n",
        "verb_class_count, noun_class_count = 125, 352\n",
        "class_count = (verb_class_count, noun_class_count)\n",
        "ckpt = torch.load(model_path)\n",
        "#model = TSN(\n",
        "#model = TRN(\n",
        "model = MTRN(\n",
        "    num_class=class_count,\n",
        "    num_segments=ckpt['segment_count'],\n",
        "    modality=ckpt['modality'],\n",
        "    base_model=ckpt['arch'],\n",
        "    dropout=ckpt['args'].dropout\n",
        ")\n",
        "model.load_state_dict(ckpt['state_dict'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multi-Scale Temporal Relation Network Module in use ['8-frame relation', '7-frame relation', '6-frame relation', '5-frame relation', '4-frame relation', '3-frame relation', '2-frame relation']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoPlqfXG9nFV"
      },
      "source": [
        "import tarfile\n",
        "with tarfile.open('/content/drive/My Drive/P01_13.tar') as tar_file:\n",
        "  tar_file.extractall('./P01_13/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxxSaFup9uZL"
      },
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def create_dataset(image_dir, frame_limit = 1000):\n",
        "    imgs = []\n",
        "    for i, image_name in enumerate(os.listdir(image_dir)):\n",
        "        if i > frame_limit:\n",
        "          break\n",
        "        image = Image.open(image_dir + image_name)\n",
        "        image = transforms.ToTensor()(image)\n",
        "        image = np.array(image)\n",
        "        imgs.append(image)\n",
        "    images = np.array(imgs)\n",
        "    dataset = transforms.ToTensor()(images[0])\n",
        "    for i in range(1,len(images)):\n",
        "      torch.cat((dataset, transforms.ToTensor()(images[i])))\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZBEq7NY-HWj"
      },
      "source": [
        "img_dir = 'P01_13/'\n",
        "images = create_dataset(img_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbiQo7_f-Z6B"
      },
      "source": [
        "output = model(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7TdUmlH-gkA"
      },
      "source": [
        "def obtain_ids(output):\n",
        "  verbids = []\n",
        "  nounids = []\n",
        "\n",
        "  for i in range(len(output[0])):\n",
        "    verbids.append(int(torch.argmax(output[0][i])))\n",
        "\n",
        "  for i in range(len(output[1])):\n",
        "    nounids.append(int(torch.argmax(output[1][i])))\n",
        "\n",
        "  return verbids, nounids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp8HZxdg_xtK"
      },
      "source": [
        "import csv\n",
        "\n",
        "def obtain_action(verbids, nounids):\n",
        "  nouns = []\n",
        "  verbs = []\n",
        "  noun_file = '/content/drive/My Drive/epic-kitchens-55-annotations-master/EPIC_noun_classes.csv'\n",
        "  verb_file = '/content/drive/My Drive/epic-kitchens-55-annotations-master/EPIC_verb_classes.csv'\n",
        "\n",
        "  with open(noun_file) as n_f:\n",
        "    noun_classes = []\n",
        "    temp_noun = csv.reader(n_f)\n",
        "    for row in temp_noun:\n",
        "      noun_classes.append(row)\n",
        "    for i in range(len(nounids)):\n",
        "      nouns.append(noun_classes[nounids[i]+1][1])\n",
        "\n",
        "  with open(verb_file) as v_f:\n",
        "    verb_classes = []\n",
        "    temp_verb = csv.reader(v_f)\n",
        "    for row in temp_verb:\n",
        "      verb_classes.append(row)\n",
        "    for i in range(len(verbids)):\n",
        "      verbs.append(verb_classes[verbids[i]+1][1])\n",
        "  return verbs, nouns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFopbVA0_4vx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "33d18a59-21cc-4409-9a49-2c519ce426e6"
      },
      "source": [
        "testverbids, testnounids = obtain_ids(output)\n",
        "testverb, testnoun = obtain_action(testverbids, testnounids)\n",
        "\n",
        "print(testverbids)\n",
        "print(testnounids)\n",
        "print(testverb)\n",
        "print(testnoun)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 4, 4, 4, 4, 1, 3, 0, 2, 1, 0, 0, 0, 0, 0, 1, 4, 4, 2]\n",
            "[72, 11, 4, 19, 7, 1, 8, 11, 43, 16, 1, 11, 1, 4, 4, 63, 32, 11, 82]\n",
            "['put', 'wash', 'wash', 'wash', 'wash', 'put', 'close', 'take', 'open', 'put', 'take', 'take', 'take', 'take', 'take', 'put', 'wash', 'wash', 'open']\n",
            "['leaf', 'lid', 'plate', 'board:chopping', 'spoon', 'pan', 'cupboard', 'lid', 'milk', 'glass', 'pan', 'lid', 'pan', 'plate', 'plate', 'tray', 'cloth', 'lid', 'machine:washing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VcDDiVJ229n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5914d683-99a2-4026-fbda-49f435cd6795"
      },
      "source": [
        "from torchvision.transforms import Compose\n",
        "from transforms import GroupScale, GroupCenterCrop, GroupOverSample, Stack, ToTorchFormatTensor, GroupNormalize\n",
        "\n",
        "crop_count = 10\n",
        "net = model\n",
        "backbone_arch = 'resnet50'\n",
        "\n",
        "if crop_count == 1:\n",
        "    cropping = Compose([\n",
        "        GroupScale(net.scale_size),\n",
        "        GroupCenterCrop(net.input_size),\n",
        "    ])\n",
        "elif crop_count == 10:\n",
        "    cropping = GroupOverSample(net.input_size, net.scale_size)\n",
        "else:\n",
        "    raise ValueError(\"Only 1 and 10 crop_count are supported while we got {}\".format(crop_count))\n",
        "\n",
        "transform = Compose([\n",
        "    cropping,\n",
        "    Stack(roll=backbone_arch == 'BNInception'),\n",
        "    ToTorchFormatTensor(div=backbone_arch != 'BNInception'),\n",
        "    GroupNormalize(net.input_mean, net.input_std),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:257: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De3uNFpwDWl_"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CookingDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    label_file = '/content/drive/My Drive/COOKING_action_labels.csv'\n",
        "    #label_file = '/content/drive/My Drive/COOKING_action_labels_20.csv'\n",
        "    cooking_dir = '/content/drive/My Drive/COOKING POV/'\n",
        "    image_names = sorted(os.listdir(cooking_dir))\n",
        "\n",
        "    xy = np.loadtxt(label_file, delimiter = ',', dtype = np.float32, skiprows = 1, usecols = (0,1,2,3))\n",
        "    self.y = torch.from_numpy(xy[:,[2,3]])\n",
        "    self.nsamples = xy.shape[0]\n",
        "\n",
        "    start_frames = xy[:, [0]].astype(int)\n",
        "    stop_frames = xy[:, [1]].astype(int)\n",
        "    self.x = []\n",
        "    for i in range(self.nsamples):\n",
        "    #for i in range(2):\n",
        "      images = []\n",
        "      for j in range(start_frames[i][0], stop_frames[i][0]):\n",
        "        image = Image.open(cooking_dir + image_names[j])\n",
        "        #image = image.resize((114,64))\n",
        "        images.append(image)\n",
        "      action = transform(images)\n",
        "      self.x.append(action)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.x[index], self.y[index]\n",
        "    \n",
        "  def __len__(self):\n",
        "    return self.nsamples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkubwXgMEaP6"
      },
      "source": [
        "cooking_dataset = CookingDataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSjW93YBEr_0"
      },
      "source": [
        "cooking_dataloader = DataLoader(dataset=cooking_dataset, batch_size=1, shuffle=False, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enew-UigE_dX"
      },
      "source": [
        "def custom_loss(prediction, label):\n",
        "  lab1 = np.zeros(len(prediction[0][0]))\n",
        "  lab2 = np.zeros(len(prediction[1][0]))\n",
        "  lab1[int(label[0][0])] = 1\n",
        "  lab2[int(label[0][1])] = 1\n",
        "  lab1 = torch.from_numpy(lab1)\n",
        "  lab2 = torch.from_numpy(lab2)\n",
        "\n",
        "  mse1 = torch.mean((lab1 - prediction[0][0])**2)\n",
        "  mse2 = torch.mean((lab2 - prediction[1][0])**2)\n",
        "\n",
        "  loss = torch.mean(torch.tensor([mse1, mse2]))\n",
        "  loss.requires_grad = True\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yexwvqeqFA8y"
      },
      "source": [
        "import torch.nn as nn\n",
        "from statistics import mean\n",
        "\n",
        "learning_rate = 0.01\n",
        "num_epochs = 10\n",
        "\n",
        "#loss = nn.MSELoss()\n",
        "optimiser = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = []\n",
        "  for i, (actions, label) in enumerate(cooking_dataloader):\n",
        "    prediction = model(actions)\n",
        "\n",
        "    loss = custom_loss(prediction, label)\n",
        "    epoch_loss.append(float(loss))\n",
        "    loss.backward()\n",
        "\n",
        "    optimiser.step()\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "  epoch_loss = np.array(epoch_loss)\n",
        "  epoch_loss = mean(epoch_loss)\n",
        "  print(f'epoch {epoch+1}/{num_epochs}: loss = {epoch_loss:.8f}')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}